{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_rf_cifar_classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlHRXmSliJ_0",
        "outputId": "378f2bf4-c9f0-42e9-a1a9-86d3a0c8c12d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        }
      },
      "source": [
        "!pip install scipy==1.1.0\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 140kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.18.5)\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "Successfully installed scipy-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhAjQ3aVjgx1",
        "outputId": "94fd67e0-148b-4074-97ed-604045e2a2d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!tar -xzvf cifar-10-python.tar.gz\n",
        "!rm cifar-10-python.tar.gz "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-06 17:15:24--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  46.4MB/s    in 3.8s    \n",
            "\n",
            "2020-11-06 17:15:28 (42.5 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xc3_QdWhTpN"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from builtins import range\n",
        "from six.moves import cPickle as pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.misc import imread\n",
        "import platform\n",
        "\n",
        "def load_pickle(f):\n",
        "    version = platform.python_version_tuple()\n",
        "    if version[0] == '2':\n",
        "        return  pickle.load(f)\n",
        "    elif version[0] == '3':\n",
        "        return  pickle.load(f, encoding='latin1')\n",
        "    raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "    \"\"\" load single batch of cifar \"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        datadict = load_pickle(f)\n",
        "        X = datadict['data']\n",
        "        Y = datadict['labels']\n",
        "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "        Y = np.array(Y)\n",
        "        return X, Y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "    \"\"\" load all of cifar \"\"\"\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for b in range(1,6):\n",
        "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "        X, Y = load_CIFAR_batch(f)\n",
        "        xs.append(X)\n",
        "        ys.append(Y)\n",
        "    Xtr = np.concatenate(xs)\n",
        "    Ytr = np.concatenate(ys)\n",
        "    del X, Y\n",
        "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "    return Xtr, Ytr, Xte, Yte\n",
        "\n",
        "\n",
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000,\n",
        "                     subtract_mean=True):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for classifiers. These are the same steps as we used for the SVM, but\n",
        "    condensed to a single function.\n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "\n",
        "    # Subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalize the data: subtract the mean image\n",
        "    if subtract_mean:\n",
        "        mean_image = np.mean(X_train, axis=0)\n",
        "        X_train -= mean_image\n",
        "        X_val -= mean_image\n",
        "        X_test -= mean_image\n",
        "\n",
        "    # Transpose so that channels come first\n",
        "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
        "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
        "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
        "\n",
        "    # Package data into a dictionary\n",
        "    return {\n",
        "      'X_train': X_train, 'y_train': y_train,\n",
        "      'X_val': X_val, 'y_val': y_val,\n",
        "      'X_test': X_test, 'y_test': y_test,\n",
        "    }\n",
        "\n",
        "\n",
        "def load_tiny_imagenet(path, dtype=np.float32, subtract_mean=True):\n",
        "    \"\"\"\n",
        "    Load TinyImageNet. Each of TinyImageNet-100-A, TinyImageNet-100-B, and\n",
        "    TinyImageNet-200 have the same directory structure, so this can be used\n",
        "    to load any of them.\n",
        "\n",
        "    Inputs:\n",
        "    - path: String giving path to the directory to load.\n",
        "    - dtype: numpy datatype used to load the data.\n",
        "    - subtract_mean: Whether to subtract the mean training image.\n",
        "\n",
        "    Returns: A dictionary with the following entries:\n",
        "    - class_names: A list where class_names[i] is a list of strings giving the\n",
        "      WordNet names for class i in the loaded dataset.\n",
        "    - X_train: (N_tr, 3, 64, 64) array of training images\n",
        "    - y_train: (N_tr,) array of training labels\n",
        "    - X_val: (N_val, 3, 64, 64) array of validation images\n",
        "    - y_val: (N_val,) array of validation labels\n",
        "    - X_test: (N_test, 3, 64, 64) array of testing images.\n",
        "    - y_test: (N_test,) array of test labels; if test labels are not available\n",
        "      (such as in student code) then y_test will be None.\n",
        "    - mean_image: (3, 64, 64) array giving mean training image\n",
        "    \"\"\"\n",
        "    # First load wnids\n",
        "    with open(os.path.join(path, 'wnids.txt'), 'r') as f:\n",
        "        wnids = [x.strip() for x in f]\n",
        "\n",
        "    # Map wnids to integer labels\n",
        "    wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n",
        "\n",
        "    # Use words.txt to get names for each class\n",
        "    with open(os.path.join(path, 'words.txt'), 'r') as f:\n",
        "        wnid_to_words = dict(line.split('\\t') for line in f)\n",
        "        for wnid, words in wnid_to_words.items():\n",
        "            wnid_to_words[wnid] = [w.strip() for w in words.split(',')]\n",
        "    class_names = [wnid_to_words[wnid] for wnid in wnids]\n",
        "\n",
        "    # Next load training data.\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for i, wnid in enumerate(wnids):\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print('loading training data for synset %d / %d'\n",
        "                  % (i + 1, len(wnids)))\n",
        "        # To figure out the filenames we need to open the boxes file\n",
        "        boxes_file = os.path.join(path, 'train', wnid, '%s_boxes.txt' % wnid)\n",
        "        with open(boxes_file, 'r') as f:\n",
        "            filenames = [x.split('\\t')[0] for x in f]\n",
        "        num_images = len(filenames)\n",
        "\n",
        "        X_train_block = np.zeros((num_images, 3, 64, 64), dtype=dtype)\n",
        "        y_train_block = wnid_to_label[wnid] * \\\n",
        "                        np.ones(num_images, dtype=np.int64)\n",
        "        for j, img_file in enumerate(filenames):\n",
        "            img_file = os.path.join(path, 'train', wnid, 'images', img_file)\n",
        "            img = imread(img_file)\n",
        "            if img.ndim == 2:\n",
        "        ## grayscale file\n",
        "                img.shape = (64, 64, 1)\n",
        "            X_train_block[j] = img.transpose(2, 0, 1)\n",
        "        X_train.append(X_train_block)\n",
        "        y_train.append(y_train_block)\n",
        "\n",
        "    # We need to concatenate all training data\n",
        "    X_train = np.concatenate(X_train, axis=0)\n",
        "    y_train = np.concatenate(y_train, axis=0)\n",
        "\n",
        "    # Next load validation data\n",
        "    with open(os.path.join(path, 'val', 'val_annotations.txt'), 'r') as f:\n",
        "        img_files = []\n",
        "        val_wnids = []\n",
        "        for line in f:\n",
        "            img_file, wnid = line.split('\\t')[:2]\n",
        "            img_files.append(img_file)\n",
        "            val_wnids.append(wnid)\n",
        "        num_val = len(img_files)\n",
        "        y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])\n",
        "        X_val = np.zeros((num_val, 3, 64, 64), dtype=dtype)\n",
        "        for i, img_file in enumerate(img_files):\n",
        "            img_file = os.path.join(path, 'val', 'images', img_file)\n",
        "            img = imread(img_file)\n",
        "            if img.ndim == 2:\n",
        "                img.shape = (64, 64, 1)\n",
        "            X_val[i] = img.transpose(2, 0, 1)\n",
        "\n",
        "    # Next load test images\n",
        "    # Students won't have test labels, so we need to iterate over files in the\n",
        "    # images directory.\n",
        "    img_files = os.listdir(os.path.join(path, 'test', 'images'))\n",
        "    X_test = np.zeros((len(img_files), 3, 64, 64), dtype=dtype)\n",
        "    for i, img_file in enumerate(img_files):\n",
        "        img_file = os.path.join(path, 'test', 'images', img_file)\n",
        "        img = imread(img_file)\n",
        "        if img.ndim == 2:\n",
        "            img.shape = (64, 64, 1)\n",
        "        X_test[i] = img.transpose(2, 0, 1)\n",
        "\n",
        "    y_test = None\n",
        "    y_test_file = os.path.join(path, 'test', 'test_annotations.txt')\n",
        "    if os.path.isfile(y_test_file):\n",
        "        with open(y_test_file, 'r') as f:\n",
        "            img_file_to_wnid = {}\n",
        "            for line in f:\n",
        "                line = line.split('\\t')\n",
        "                img_file_to_wnid[line[0]] = line[1]\n",
        "        y_test = [wnid_to_label[img_file_to_wnid[img_file]]\n",
        "                  for img_file in img_files]\n",
        "        y_test = np.array(y_test)\n",
        "\n",
        "    mean_image = X_train.mean(axis=0)\n",
        "    if subtract_mean:\n",
        "        X_train -= mean_image[None]\n",
        "        X_val -= mean_image[None]\n",
        "        X_test -= mean_image[None]\n",
        "\n",
        "    return {\n",
        "      'class_names': class_names,\n",
        "      'X_train': X_train,\n",
        "      'y_train': y_train,\n",
        "      'X_val': X_val,\n",
        "      'y_val': y_val,\n",
        "      'X_test': X_test,\n",
        "      'y_test': y_test,\n",
        "      'class_names': class_names,\n",
        "      'mean_image': mean_image,\n",
        "    }\n",
        "\n",
        "\n",
        "def load_models(models_dir):\n",
        "    \"\"\"\n",
        "    Load saved models from disk. This will attempt to unpickle all files in a\n",
        "    directory; any files that give errors on unpickling (such as README.txt)\n",
        "    will be skipped.\n",
        "\n",
        "    Inputs:\n",
        "    - models_dir: String giving the path to a directory containing model files.\n",
        "      Each model file is a pickled dictionary with a 'model' field.\n",
        "\n",
        "    Returns:\n",
        "    A dictionary mapping model file names to models.\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    for model_file in os.listdir(models_dir):\n",
        "        with open(os.path.join(models_dir, model_file), 'rb') as f:\n",
        "            try:\n",
        "                models[model_file] = load_pickle(f)['model']\n",
        "            except pickle.UnpicklingError:\n",
        "                continue\n",
        "    return models\n",
        "\n",
        "\n",
        "def load_imagenet_val(num=None):\n",
        "    \"\"\"Load a handful of validation images from ImageNet.\n",
        "\n",
        "    Inputs:\n",
        "    - num: Number of images to load (max of 25)\n",
        "\n",
        "    Returns:\n",
        "    - X: numpy array with shape [num, 224, 224, 3]\n",
        "    - y: numpy array of integer image labels, shape [num]\n",
        "    - class_names: dict mapping integer label to class name\n",
        "    \"\"\"\n",
        "    imagenet_fn = 'cs231n/datasets/imagenet_val_25.npz'\n",
        "    if not os.path.isfile(imagenet_fn):\n",
        "      print('file %s not found' % imagenet_fn)\n",
        "      print('Run the following:')\n",
        "      print('cd cs231n/datasets')\n",
        "      print('bash get_imagenet_val.sh')\n",
        "      assert False, 'Need to download imagenet_val_25.npz'\n",
        "    f = np.load(imagenet_fn)\n",
        "    X = f['X']\n",
        "    y = f['y']\n",
        "    class_names = f['label_map'].item()\n",
        "    if num is not None:\n",
        "        X = X[:num]\n",
        "        y = y[:num]\n",
        "    return X, y, class_names\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VKvqeV5iDZ7",
        "outputId": "f336f9ce-d749-4280-9a83-c3fd3a2b3bb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cifar10_dir = '/content/cifar-10-batches-py'\n",
        "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "\n",
        "# As a sanity check, we print out the size of the training and test data.\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data shape:  (50000, 32, 32, 3)\n",
            "Training labels shape:  (50000,)\n",
            "Test data shape:  (10000, 32, 32, 3)\n",
            "Test labels shape:  (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPJm1tX3jycm",
        "outputId": "14f50553-1363-4f49-b4da-3b1819bfe8c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#reshape the data \n",
        "X_train=X_train.reshape(50000,-1)\n",
        "X_test= X_test.reshape(10000,-1)\n",
        "# As a sanity check, we print out the size of the training and test data.\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data shape:  (50000, 3072)\n",
            "Training labels shape:  (50000,)\n",
            "Test data shape:  (10000, 3072)\n",
            "Test labels shape:  (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uveUwpb7j6Hm",
        "outputId": "39d6eea2-5efe-4766-b23c-34a611568cc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load scikit's random forest classifier library\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a random forest Classifier. By convention, clf means 'Classifier'\n",
        "clf = RandomForestClassifier(n_estimators=20,n_jobs=2, random_state=0)\n",
        "\n",
        "# Train the Classifier to take the training features and learn how they relate\n",
        "# to the training y (the species)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=2,\n",
              "                       oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdBaLNP1k1xB",
        "outputId": "9ff3ef39-61b3-4afc-a1ef-2081a9ec0503",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "preds= clf.predict(X_test)\n",
        "true_classes=np.sum(preds==y_test)\n",
        "total= X_test.shape[0]\n",
        "print(\"accuracy\",true_classes/total*100)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 40.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiPjWFg4lPt-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmiAMLjPmDvs"
      },
      "source": [
        "Remember this is blind random forest on image pixels. Image cols dont have any statistical pattern. Better performance could be obtained from calculating unchanging representation for each image, and putting that into the col. For eg, use a CNN as a feature extractor for random forest. But, then their would be no point in using rf at all. \n",
        "\n",
        "Followup :\n",
        "**can you think of any other statistics you can calculate for cifar image??**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDBBBy7FmhP9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}